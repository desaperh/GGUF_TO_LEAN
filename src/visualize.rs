/// Example: Visualize GGUF model architecture
use gguf_to_lean::{GGUFReader, ArchitectureAnalyzer};
use std::env;

fn main() -> Result<(), Box<dyn std::error::Error>> {
    let args: Vec<String> = env::args().collect();
    
    if args.len() < 2 {
        eprintln!("Usage: {} <model.gguf>", args[0]);
        return Ok(());
    }
    
    let gguf_path = &args[1];
    
    println!("ðŸ“‚ Reading GGUF file: {}", gguf_path);
    
    // Parse GGUF
    let mut reader = GGUFReader::open(gguf_path)?;
    reader.parse()?;
    
    // Analyze architecture
    let analyzer = ArchitectureAnalyzer::new(
        reader.tensor_infos(),
        reader.metadata(),
    );
    let architecture = analyzer.analyze();
    
    // Print visualization
    println!("\n{}", "=".repeat(80));
    println!("MODEL ARCHITECTURE VISUALIZATION");
    println!("{}", "=".repeat(80));
    println!();
    
    // Configuration
    println!("ðŸ“Š MODEL CONFIGURATION");
    println!("{}", "-".repeat(40));
    println!("  Layers:           {}", architecture.config.n_layers);
    println!("  Attention heads:  {}", architecture.config.n_heads);
    println!("  KV heads:         {}", architecture.config.n_kv_heads);
    println!("  Hidden size:      {}", architecture.config.hidden_size);
    println!("  Vocab size:       {}", architecture.config.vocab_size);
    println!("  Context length:   {}", architecture.config.context_length);
    println!("  Intermediate:     {}", architecture.config.intermediate_size);
    println!("  Architecture:     {}", architecture.config.architecture);
    println!();
    
    // Embedding
    if let Some(ref emb) = architecture.embedding {
        println!("ðŸ”¤ EMBEDDING LAYER");
        println!("{}", "-".repeat(40));
        println!("  Tensor: {}", emb.name);
        println!("  Shape:  {:?}", emb.dimensions);
        println!("  Type:   {:?}", emb.tensor_type);
        println!();
    }
    
    // Layers
    println!("ðŸ”— TRANSFORMER LAYERS");
    println!("{}", "-".repeat(40));
    
    for (i, layer) in architecture.layers.iter().enumerate().take(3) {
        println!("\n  Layer {}", i);
        println!("  {}", "â”€".repeat(36));
        println!("  Input:  [batch, seq_len, {}]", architecture.config.hidden_size);
        println!();
        
        if layer.has_attention() {
            println!("  â”Œâ”€ Attention Block");
            
            if layer.has_component("attn_norm") {
                println!("  â”‚  â”œâ”€ RMS Norm");
            }
            
            println!("  â”‚  â”œâ”€ Multi-Head Attention ({} heads)", architecture.config.n_heads);
            
            if layer.has_component("q_proj") {
                let comp = &layer.components["q_proj"];
                println!("  â”‚  â”‚  â”œâ”€ Query:  {:?}", comp.dimensions);
            }
            if layer.has_component("k_proj") {
                let comp = &layer.components["k_proj"];
                println!("  â”‚  â”‚  â”œâ”€ Key:    {:?}", comp.dimensions);
            }
            if layer.has_component("v_proj") {
                let comp = &layer.components["v_proj"];
                println!("  â”‚  â”‚  â”œâ”€ Value:  {:?}", comp.dimensions);
            }
            
            println!("  â”‚  â”‚  â””â”€ Attention: softmax(Q @ K^T) @ V");
            
            if layer.has_component("o_proj") {
                let comp = &layer.components["o_proj"];
                println!("  â”‚  â””â”€ Output: {:?}", comp.dimensions);
            }
            
            println!("  â”‚  â””â”€ Residual: input + attention_output");
            println!();
        }
        
        if layer.has_mlp() {
            println!("  â””â”€ MLP Block");
            
            if layer.has_component("ffn_norm") {
                println!("     â”œâ”€ RMS Norm");
            }
            
            println!("     â”œâ”€ SwiGLU Activation");
            
            if layer.has_component("gate_proj") {
                let comp = &layer.components["gate_proj"];
                println!("     â”‚  â”œâ”€ Gate:  {:?}", comp.dimensions);
            }
            if layer.has_component("up_proj") {
                let comp = &layer.components["up_proj"];
                println!("     â”‚  â”œâ”€ Up:    {:?}", comp.dimensions);
            }
            if layer.has_component("down_proj") {
                let comp = &layer.components["down_proj"];
                println!("     â”‚  â””â”€ Down:  {:?}", comp.dimensions);
            }
            
            println!("     â””â”€ Residual: residual1 + mlp_output");
        }
        
        println!();
        println!("  Output: [batch, seq_len, {}]", architecture.config.hidden_size);
    }
    
    if architecture.layers.len() > 3 {
        println!("\n  ... ({} more layers)", architecture.layers.len() - 3);
    }
    
    // Output
    if let Some(ref out) = architecture.output {
        println!("\nðŸ“¤ OUTPUT LAYER");
        println!("{}", "-".repeat(40));
        println!("  Tensor: {}", out.name);
        println!("  Shape:  {:?}", out.dimensions);
        println!("  Type:   {:?}", out.tensor_type);
        println!();
    }
    
    // Statistics
    println!("ðŸ“ˆ MODEL STATISTICS");
    println!("{}", "-".repeat(40));
    
    let total_tensors = reader.tensor_infos().len();
    let total_params: u64 = reader.tensor_infos()
        .iter()
        .map(|info| info.n_elements() as u64)
        .sum();
    
    println!("  Total tensors:    {}", total_tensors);
    println!("  Total parameters: ~{:.2}B", total_params as f64 / 1e9);
    println!();
    
    // Data flow
    println!("ðŸŒŠ DATA FLOW");
    println!("{}", "-".repeat(40));
    println!();
    println!("  Text Input");
    println!("      â†“");
    println!("  Tokenization");
    println!("      â†“");
    println!("  Embedding: [{}, {}]", 
             architecture.config.vocab_size,
             architecture.config.hidden_size);
    println!("      â†“");
    
    for i in 0..std::cmp::min(3, architecture.layers.len()) {
        println!("  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
        println!("  â”‚   Layer {:2}      â”‚", i);
        println!("  â”‚   - Attention   â”‚");
        println!("  â”‚   - MLP         â”‚");
        println!("  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜");
        println!("      â†“");
    }
    
    if architecture.layers.len() > 3 {
        println!("      ... ({} more layers)", architecture.layers.len() - 3);
        println!("      â†“");
    }
    
    println!("  LM Head: [{}, {}]",
             architecture.config.hidden_size,
             architecture.config.vocab_size);
    println!("      â†“");
    println!("  Logits â†’ argmax/sampling");
    println!("      â†“");
    println!("  Text Output");
    println!();
    
    Ok(())
}
